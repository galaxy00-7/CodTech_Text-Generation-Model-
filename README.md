# 📝 Text Generation using GPT-2 - CodTech Internship Task 4

This project implements a **Text Generation Model** using the pre-trained **GPT-2** language model from Hugging Face Transformers. Built as part of **CodTech Internship Task 4**, the project demonstrates how transformer-based models can generate coherent paragraphs on various topics with just a simple prompt.

---

## 🧠 What is GPT-2?

**GPT-2 (Generative Pre-trained Transformer 2)** is a transformer-based language model developed by OpenAI. It can generate human-like text based on a prompt, making it useful for tasks like content generation, dialogue systems, and summarization.

---

## 📁 Project Structure
<pre>
├── Text_Generation_GPT2_Task4.ipynb   # Main Jupyter notebook containing all code and explanations  
├── requirements.txt                   # List of required Python packages  
└── README.md                          # Project documentation  
</pre>

---

## ⚙️ How It Works

1. Load and initialize the GPT-2 model and tokenizer using Hugging Face Transformers.
2. Define a text generation function that takes a topic or sentence prompt.
3. Generate and display coherent paragraphs based on various input topics.

---

## 🚀 How to Run

### 1. Requirements

Install the required libraries:
```bash
pip install -r requirements.txt
```

### 2. Open the Notebook

Launch `Text_Generation_GPT2_Task4.ipynb` using Jupyter Notebook or VS Code.

### 3. Run the Notebook

Execute the cells sequentially to:
- Load the GPT-2 model
- Define the generation function
- Generate text for sample topics

---

### 📚 Technologies Used
• Python  
• Hugging Face Transformers  
• PyTorch  
• Jupyter Notebook  

---

### 📦 Additional Files

**`requirements.txt`**  
Contains the list of required libraries (`transformers`, `torch`) to help anyone quickly set up the project environment using:
```bash
pip install -r requirements.txt
```

---

### 👨‍💻 Author

Raman Kumar   
CodTech Internship - AI Track   
July 2025  