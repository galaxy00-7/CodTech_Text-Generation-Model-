# ğŸ“ Text Generation using GPT-2 - CodTech Internship Task 4

This project implements a **Text Generation Model** using the pre-trained **GPT-2** language model from Hugging Face Transformers. Built as part of **CodTech Internship Task 4**, the project demonstrates how transformer-based models can generate coherent paragraphs on various topics with just a simple prompt.

---

## ğŸ§  What is GPT-2?

**GPT-2 (Generative Pre-trained Transformer 2)** is a transformer-based language model developed by OpenAI. It can generate human-like text based on a prompt, making it useful for tasks like content generation, dialogue systems, and summarization.

---

## ğŸ“ Project Structure
<pre>
â”œâ”€â”€ text_generation.ipynb              # Main Jupyter notebook containing all code and explanations  
â”œâ”€â”€ requirements.txt                   # List of required Python packages  
â””â”€â”€ README.md                          # Project documentation  
</pre>

---

## âš™ï¸ How It Works

1. Load and initialize the GPT-2 model and tokenizer using Hugging Face Transformers.
2. Define a text generation function that takes a topic or sentence prompt.
3. Generate and display coherent paragraphs based on various input topics.

---

## ğŸš€ How to Run

### 1. Requirements

Install the required libraries:
```bash
pip install -r requirements.txt
```

### 2. Open the Notebook

Launch `text_generation.ipynb ` using Jupyter Notebook or VS Code.

### 3. Run the Notebook

Execute the cells sequentially to:
- Load the GPT-2 model
- Define the generation function
- Generate text for sample topics

---

### ğŸ“š Technologies Used
â€¢ Python  
â€¢ Hugging Face Transformers  
â€¢ PyTorch  
â€¢ Jupyter Notebook  

---

### ğŸ“¦ Additional Files

**`requirements.txt`**  
Contains the list of required libraries (`transformers`, `torch`) to help anyone quickly set up the project environment using:
```bash
pip install -r requirements.txt
```

---

### ğŸ‘¨â€ğŸ’» Author

Raman Kumar   
CodTech Internship 
July 2025  
